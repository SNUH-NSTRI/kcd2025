"""Service for generating reports using LLM."""

import asyncio
import json
from pathlib import Path
from typing import AsyncGenerator

# Use OpenRouter with OpenAI's library
# Make sure to install it: pip install openai
# Set the OPENROUTER_API_KEY environment variable
import openai

from rwe_api.config import settings  # Centralized config


def _get_llm_client() -> openai.AsyncOpenAI:
    """Get or create LLM client with OpenRouter configuration."""
    openrouter_api_key = settings.OPENROUTER_API_KEY
    if not openrouter_api_key:
        raise ValueError("OPENROUTER_API_KEY environment variable is required")

    return openai.AsyncOpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=openrouter_api_key,
    )

async def stream_llm_report(project_path: Path, cohort_path: Path, display_medication: str) -> AsyncGenerator[str, None]:
    """
    Loads data, builds a prompt, and streams a report from an LLM.

    Args:
        project_path: Path to NCT project directory.
        cohort_path: Path to specific cohort directory.
        display_medication: Original medication name for display.

    Yields:
        String chunks of the generated markdown report.
    """
    try:
        # --- 1. Load and structure data for the LLM ---
        context_data = await _build_llm_context(project_path, cohort_path, display_medication)
        prompt = _create_prompt(context_data)

        # --- 2. Stream static header content immediately ---
        header = f"# Clinical Trial Emulation Report: {context_data['study_overview']['study_name']}\n\n"
        header += f"**NCT ID:** {context_data['study_overview']['nct_id']} | "
        header += f"**Medication:** {context_data['study_overview']['medication']}\n\n---\n\n"
        yield header

        # --- 3. Call LLM and stream the response ---
        # Using OpenRouter with GPT-4 Turbo
        llm_client = _get_llm_client()
        stream = await llm_client.chat.completions.create(
            model="openai/gpt-4-turbo",  # OpenRouter model format
            messages=[
                {"role": "system", "content": prompt["system"]},
                {"role": "user", "content": prompt["user"]},
            ],
            stream=True,
        )
        async for chunk in stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content

        # --- 4. Stream static footer ---
        footer = "\n\n---\n\n*This report was automatically generated by the RWE Clinical Trial Emulation Platform with AI assistance.*\n"
        yield footer

    except FileNotFoundError as e:
        yield f"\n**Error:** Could not generate report. A required data file is missing: {e.filename}\n"
    except Exception as e:
        yield f"\n**Error:** An unexpected error occurred during report generation: {str(e)}\n"


def _load_nct_papers(project_path: Path) -> list[dict]:
    """Load NCT papers from lit directory."""
    papers = []
    lit_dir = project_path / "lit"

    if not lit_dir.exists():
        return papers

    # Find all markdown files (excluding README)
    paper_files = [f for f in lit_dir.glob("*.md") if f.name.lower() != "readme.md"]

    for paper_file in paper_files:
        try:
            content = paper_file.read_text(encoding="utf-8")

            # Extract title and URL from markdown header
            title = "Unknown Title"
            url = ""
            published_time = ""

            lines = content.split('\n')
            for i, line in enumerate(lines[:10]):  # Check first 10 lines
                if line.startswith("Title:"):
                    title = line.replace("Title:", "").strip()
                elif line.startswith("URL Source:"):
                    url = line.replace("URL Source:", "").strip()
                elif line.startswith("Published Time:"):
                    published_time = line.replace("Published Time:", "").strip()

            papers.append({
                "filename": paper_file.name,
                "title": title,
                "url": url,
                "published_time": published_time,
                "content": content[:50000]  # Limit to first 50k chars to avoid token overflow
            })
        except Exception as e:
            # Log error but continue with other papers
            print(f"Warning: Failed to load paper {paper_file.name}: {e}")
            continue

    return papers


async def _build_llm_context(project_path: Path, cohort_path: Path, display_medication: str) -> dict:
    """Loads data from files and structures it into a dictionary for the LLM."""
    metadata_file = cohort_path / "metadata.json"
    stat_report_file = cohort_path / "outputs" / "statistician_report.md"

    if not metadata_file.exists():
        raise FileNotFoundError(None, None, str(metadata_file))

    with open(metadata_file, "r") as f:
        metadata = json.load(f)

    # Load the full statistical report for LLM context
    # Pass the complete analysis so LLM can interpret and summarize appropriately
    full_stat_report = "Statistical analysis not available."
    if stat_report_file.exists():
        full_stat_report = stat_report_file.read_text(encoding="utf-8")

    # Load NCT papers from lit directory
    papers = _load_nct_papers(project_path)

    context = {
        "study_overview": {
            "nct_id": project_path.name,
            "medication": display_medication,
            "study_name": metadata.get('cohort_name', 'N/A'),
            "mimic_version": metadata.get('mimic_version', 'N/A'),
        },
        "cohort_summary": {
            "initial_patients": metadata.get('attrition_funnel', [{}])[0].get('patients_remaining', 0),
            "final_patients": metadata.get('total_patients', 0),
            "treatment_group_size": metadata.get('treatment_assignment', {}).get('treatment_group', 0),
            "control_group_size": metadata.get('treatment_assignment', {}).get('control_group', 0),
        },
        "attrition_funnel_summary": [
            {"step": s["step"], "description": s["description"], "remaining": s["patients_remaining"]}
            for s in metadata.get("attrition_funnel", [])
        ],
        "statistical_analysis_full_report": full_stat_report,
        "nct_papers": papers,
    }
    return context


def _create_prompt(context_data: dict) -> dict:
    """Creates the system and user prompts for the LLM."""
    system_prompt = (
        "You are a senior medical writer specializing in clinical trial reports for busy clinicians. "
        "Your reports are known for being concise, clinically relevant, and easy to read during rounds. "
        "You have deep expertise in interpreting statistical analysis and translating complex data into actionable clinical insights."
    )

    # Format papers for prompt
    papers_section = ""
    if context_data.get("nct_papers"):
        papers_section = "\n\n**Clinical Trial Papers:**\n"
        for paper in context_data["nct_papers"]:
            papers_section += f"\n### {paper['title']}\n"
            papers_section += f"**Source:** {paper['url']}\n"
            papers_section += f"**Published:** {paper['published_time']}\n\n"
            papers_section += f"{paper['content'][:30000]}\n\n---\n"  # Include substantial content

    user_prompt = f"""
You have been provided with complete clinical trial emulation data including:
- Original clinical trial papers (published research)
- Study metadata and cohort characteristics
- Patient attrition funnel showing selection criteria
- Full statistical analysis report with detailed results from MIMIC-IV data

**Your Task:**
Write a concise, clinically-focused report that a busy physician can read in 3-5 minutes.
Compare the original trial results with the MIMIC-IV emulation results, highlighting similarities and differences.
Extract and summarize the most important findings from both sources, focusing on clinical implications.

**Data:**
```json
{json.dumps({k: v for k, v in context_data.items() if k != "nct_papers"}, indent=2)}
```
{papers_section}

**Writing Guidelines:**
1. **Be concise**: Summarize appropriately - don't just repeat all the data
2. **Focus on clinical relevance**: What do these results mean for patient care?
3. **Compare original vs emulation**: Highlight how MIMIC-IV results compare to the published trial
4. **Interpret statistics**: Explain what HR, CI, and p-values mean in plain clinical language
5. **Highlight key findings**: Extract the most important results from both the original paper and emulation
6. **Be actionable**: Help clinicians understand whether this treatment should be considered

**Required Structure:**
```markdown
## 1. ðŸ“‹ Executive Summary
Write 2-3 clear sentences covering:
- Study purpose and primary outcome
- Comparison of original vs emulation findings
- Clinical bottom line

Use simple language. Avoid jargon.

## 2. ðŸ”¬ Original Clinical Trial
Provide a concise summary with bullet points:
- **Study Design**: RCT/observational, setting, duration
- **Population**: Sample size, key inclusion criteria
- **Intervention**: Treatment details
- **Primary Outcome**: Main finding with statistics (HR, CI, p-value)
- **Conclusion**: What the original investigators concluded

Keep it brief. Focus on key numbers.

## 3. ðŸ’» MIMIC-IV Emulation Study
Describe the emulation process clearly:
- **Initial Cohort**: Starting patient count
- **Exclusion Steps**: Brief mention of key criteria (e.g., age, comorbidities)
- **Final Cohort**: Final patient count
- **Treatment Groups**: Treatment vs control sizes
- **Match Quality**: How well does MIMIC match the original trial?

Use clear numbers. Show the funnel visually if possible.

## 4. ðŸ“Š Comparative Results
Create a side-by-side comparison table or clear comparison:

**Mortality/Primary Outcome:**
- Original Trial: [X%] treatment vs [Y%] control (HR: [value], 95% CI: [range], p=[value])
- MIMIC Emulation: [X%] treatment vs [Y%] control (HR: [value], 95% CI: [range], p=[value])

**Concordance:**
State whether results agree or disagree. Explain any major differences.

Use tables if possible. Make comparisons crystal clear.

## 5. ðŸ’¡ Clinical Interpretation
Answer these questions for busy clinicians:
- âœ“ **Validation**: Does real-world data confirm the trial?
- âœ“ **Treatment Effect**: Is it meaningful clinically (not just statistically)?
- âœ“ **Magnitude**: How big is the benefit/harm? (NNT, absolute risk reduction)
- âœ“ **Limitations**: What are the caveats?
- âœ“ **Practice Impact**: Should this change how we treat patients?

Be direct. Give clear guidance.

## 6. ðŸŽ¯ Conclusion
1-2 sentences with clear takeaway:
- Specific recommendation for practice
- Or clear statement about evidence quality

End with actionable guidance.
```

**Important:**
- Review both the original clinical trial paper AND the `statistical_analysis_full_report` from MIMIC-IV emulation
- Focus on comparing and contrasting the two data sources
- Don't overwhelm with all statistical details - focus on what clinicians need to know
- Highlight any important differences between trial and real-world populations
"""
    return {"system": system_prompt, "user": user_prompt}
